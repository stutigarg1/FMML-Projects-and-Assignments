{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Module 8 Project: Bayesian Machine Learning**"
      ],
      "metadata": {
        "id": "v-u6tbcteeaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VVV-3/FMML_Module8_Project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ae9jHVhtZak",
        "outputId": "3fd1de97-caaf-4e3a-ca23-a17b5af7481c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FMML_Module8_Project'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 18 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (18/18), 438.62 KiB | 2.18 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfreader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ycTE-L0urLS",
        "outputId": "7251fb05-a8c5-4f2b-e4d7-b8335d111e5e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfreader in /usr/local/lib/python3.11/dist-packages (0.1.15)\n",
            "Requirement already satisfied: bitarray>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from pdfreader) (3.2.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from pdfreader) (11.1.0)\n",
            "Requirement already satisfied: pycryptodome>=3.9.9 in /usr/local/lib/python3.11/dist-packages (from pdfreader) (3.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pdfreader) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pdfreader) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"FMML_Module8_Project\")"
      ],
      "metadata": {
        "id": "N-oYJqA8taqp"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI5d79nxtejV",
        "outputId": "0fc4b777-16ee-4e48-c276-0ecb7441282d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "autohire  data\tREADME.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls autohire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyhfR5_NtoaT",
        "outputId": "281a9a08-7a8b-43e7-8a8a-c050ccc65a96"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bow.py\t    explainer.py    __init__.py  model.py\t   utils.py\n",
            "encoder.py  hyperparams.py  __main__.py  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/bow.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rxf2SiZdQ4a",
        "outputId": "989e53c8-6f62-433d-a658-62b1b449d78d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from os import stat\n",
            "import typing\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "import typing\n",
            "\n",
            "\n",
            "class BagOfWords:\n",
            "    \"\"\"\n",
            "    A type of encoder, makes\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, data: typing.Iterable) -> None:\n",
            "        \"\"\"\n",
            "        Generate the bag of words\n",
            "        :param data: an array of words, or an iterable containing arrays of words\n",
            "        \"\"\"\n",
            "        data = np.array(self.__linearize_array(data))\n",
            "        self.index_to_words = np.unique(data)\n",
            "        self.words_to_index = {w: i for i, w in enumerate(self.index_to_words)}\n",
            "\n",
            "    @classmethod\n",
            "    def __linearize_array(cls, text):\n",
            "        x = []\n",
            "        for item in text:\n",
            "            if isinstance(item, str):\n",
            "                x.append(item)\n",
            "            else:\n",
            "                x.extend(cls.__linearize_array(item))\n",
            "        return x\n",
            "\n",
            "    def __call__(self, text: typing.Iterable[str]) -> np.array:\n",
            "        return self.get_counts(text)\n",
            "\n",
            "    def __len__(self) -> int:\n",
            "        return len(self.index_to_words)\n",
            "\n",
            "    def encode_data(\n",
            "        self: \"BagOfWords\",\n",
            "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
            "    ) -> np.array:\n",
            "        \"\"\"\n",
            "        Compute the encodings of words in a new input tokenized string\n",
            "        \"\"\"\n",
            "        x = []\n",
            "        for item in text:\n",
            "            if isinstance(item, str):\n",
            "                if item in self.words_to_index:\n",
            "                    x.append(self.words_to_index[item])\n",
            "            else:\n",
            "                x.append(self.encode_data(item))\n",
            "        return x\n",
            "\n",
            "    def decode_data(self: \"BagOfWords\", encoded_text: typing.Iterable[int]):\n",
            "        if isinstance(encoded_text, int) or isinstance(encoded_text, np.int64):\n",
            "            return self.index_to_words[encoded_text]\n",
            "        else:\n",
            "            return list(map(self.decode_data, encoded_text))\n",
            "\n",
            "    def get_counts(\n",
            "        self: \"BagOfWords\",\n",
            "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Computes the counts of words in a new input tokenized string\n",
            "        \"\"\"\n",
            "        if len(text) == 0 or isinstance(text[0], str):\n",
            "            x = np.zeros(shape=len(self))\n",
            "            for word in text:\n",
            "                if word in self.words_to_index:\n",
            "                    x[self.words_to_index[word]] += 1\n",
            "            return x\n",
            "        else:\n",
            "            return np.stack([self.get_counts(item) for item in text], axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/encoder.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlcf1c7edQ8A",
        "outputId": "43e8df60-6f11-4299-8393-2883347062bd"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import numpy as np\n",
            "\n",
            "\n",
            "class LabelEncoder:\n",
            "    \"\"\"\n",
            "    Label encode a series of labels\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, data) -> None:\n",
            "        self.__training_data = data\n",
            "        self.index_to_token = list(set(data))\n",
            "        self.token_to_index = {\n",
            "            token: index for index, token in enumerate(self.index_to_token)\n",
            "        }\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.token_to_index)\n",
            "\n",
            "    @property\n",
            "    def encoded_data(self):\n",
            "        return np.array([self.token_to_index[token] for token in self.__training_data])\n",
            "\n",
            "    def encode(self, data):\n",
            "        return np.array([self.token_to_index[token] for token in data])\n",
            "\n",
            "    def decode(self, data):\n",
            "        if isinstance(data, int) or isinstance(data, np.int64):\n",
            "            return self.index_to_token[data]\n",
            "        else:\n",
            "            return np.array([self.index_to_token[index] for index in data])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/explainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmjydKPgdQ_p",
        "outputId": "8d67ff3d-f76e-4456-c4b8-3f9463eb1850"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import numpy as np\n",
            "\n",
            "from .model import BayesianMulticlassModel\n",
            "from .bow import BagOfWords\n",
            "from .encoder import LabelEncoder\n",
            "\n",
            "\n",
            "class BayesianModelExplainer(BayesianMulticlassModel):\n",
            "    \"\"\"\n",
            "    Explainer of the decision made by the base model\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, label_encoder: LabelEncoder, bag_of_words: BagOfWords) -> None:\n",
            "        super().__init__(len(label_encoder), len(bag_of_words))\n",
            "        self.bag_of_words = bag_of_words\n",
            "        self.label_encoder = label_encoder\n",
            "\n",
            "    def explain(self, text=None, label_filter=None):\n",
            "        \"\"\"\n",
            "        Visualize what are the prior probabilities of classes and which words\n",
            "        add the the likelihood of each class.\n",
            "        \"\"\"\n",
            "        class_frequencies = np.sum(self.counts, axis=1)\n",
            "        word_frequencies = np.sum(self.counts, axis=0)\n",
            "\n",
            "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
            "        likelihood = self.counts / np.expand_dims(\n",
            "            class_frequencies, axis=1\n",
            "        )  # p(word|label)\n",
            "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
            "\n",
            "        if text is not None:\n",
            "            counts_vector = self.bag_of_words.get_counts(text)\n",
            "            likelihood = np.multiply(likelihood, counts_vector)\n",
            "\n",
            "        prior_ordering = np.flip(np.argsort(prior))\n",
            "        for item in prior_ordering:\n",
            "            likelihood = likelihood / (evidence + 0.00001)\n",
            "            label = self.label_encoder.decode(item)\n",
            "            word_ids = np.flip(np.argsort(likelihood[item]))\n",
            "            word_ids = word_ids[:10]\n",
            "            if label_filter is None or label in label_filter:\n",
            "                print(f\"{label}: {' '.join(self.bag_of_words.decode_data(word_ids))}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/hyperparams.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwI0VrzudRB-",
        "outputId": "463e9011-3187-49c6-9df6-0f9119d4a6fa"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD_LENGTH_THRESHOLD = 2\n",
            "WORD_COUNT_THRESHOLD = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/__init__.py"
      ],
      "metadata": {
        "id": "92eObJO5UzB5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/__main__.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqxhJlxjdRF5",
        "outputId": "e4ae4ec8-bc41-4f73-85d4-cfc75858b9f9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import numpy as np\n",
            "\n",
            "from autohire.utils import parse_pdf, parse_resume_df\n",
            "from autohire.bow import BagOfWords\n",
            "from autohire.encoder import LabelEncoder\n",
            "from autohire.model import BayesianMulticlassModel\n",
            "from autohire.explainer import BayesianModelExplainer\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    x_train, y_train = parse_resume_df()\n",
            "    bag_of_words = BagOfWords(x_train)\n",
            "    label_encoder = LabelEncoder(y_train)\n",
            "\n",
            "    x_train = bag_of_words.get_counts(x_train)\n",
            "    y_train = label_encoder.encode(y_train)\n",
            "    model = BayesianMulticlassModel(len(label_encoder), len(bag_of_words))\n",
            "    model.fit(x_train=x_train, y_train=y_train)\n",
            "\n",
            "    x_test_input = parse_pdf(\"data/resumes/computers_2.pdf\")\n",
            "    x_test = bag_of_words.get_counts(x_test_input)\n",
            "    result = model.predict(x_train[0])\n",
            "    result = label_encoder.decode(result)\n",
            "\n",
            "    for job in result[:5]:\n",
            "        print(job)\n",
            "\n",
            "    explainable_model = BayesianModelExplainer(label_encoder, bag_of_words)\n",
            "    explainable_model.fit(x_train=x_train, y_train=y_train)\n",
            "\n",
            "    print(\n",
            "        \"\"\"\n",
            "ANALYSIS OF TRAINED PRIOR\n",
            "-------------------------\"\"\"\n",
            "    )\n",
            "    explainable_model.explain()\n",
            "\n",
            "    print(\n",
            "        \"\"\"\n",
            "ANALYSIS OF TRAINED EVIDENCE\n",
            "----------------------------\"\"\"\n",
            "    )\n",
            "    explainable_model.explain(x_test_input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcNoqxHvdaKn",
        "outputId": "a94f49c3-d819-4108-be36-80f15a6cca0e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import typing\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "class BayesianMulticlassModel:\n",
            "    \"\"\"\n",
            "    A multi-class bayesian classfier from encoded text tokens\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, num_classes, num_tokens) -> None:\n",
            "        self.counts = np.zeros(shape=(num_classes, num_tokens))\n",
            "\n",
            "    def fit(self, x_train: typing.Iterable[np.ndarray], y_train: typing.Iterable[int]):\n",
            "        for x, y in zip(x_train, y_train):\n",
            "        self.counts[y] += x\n",
            "\n",
            "    def predict(self, counts_vector):\n",
            "        class_frequencies = np.sum(self.counts, axis=1)\n",
            "        word_frequencies = np.sum(self.counts, axis=0)\n",
            "\n",
            "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
            "        likelihood = self.counts / np.expand_dims(\n",
            "            class_frequencies, axis=1\n",
            "        )  # p(word|label)\n",
            "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
            "\n",
            "        likelihood = np.multiply(likelihood, counts_vector)\n",
            "        prior = np.expand_dims(prior, axis=1)\n",
            "\n",
            "        posterior_marginal = prior * likelihood / evidence + 0.00001\n",
            "        posterior_joint = np.sum(np.log(posterior_marginal), axis=1)\n",
            "        return np.flip(np.argsort(posterior_joint))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_znzZ3JTNP1",
        "outputId": "c2df2f31-7cab-4e9d-e55e-5ba164df6c35"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy>=1.19.0\n",
            "pandas>=1.3.0\n",
            "\n",
            "black\n",
            "mypy\n",
            "pylint\n",
            "pytest\n",
            "sphinx\n",
            "sphinx-rtd-docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat autohire/utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha8yqaV6aoRE",
        "outputId": "d91c85dd-49a5-40ce-d474-04a808806407"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import re\n",
            "from collections import defaultdict\n",
            "\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from pdfreader import PDFDocument, SimplePDFViewer, document\n",
            "\n",
            "from .hyperparams import *\n",
            "\n",
            "\n",
            "def clean_text(text: str):\n",
            "    \"\"\"\n",
            "    Given text it removes all the non-character words, small words,\n",
            "    converts everything to small letters, tokenizes and returns as a list.\n",
            "    :param text: The text to be cleaned\n",
            "    \"\"\"\n",
            "    text = text.lower()\n",
            "    text = re.sub(\"[^a-z]\", \" \", text)\n",
            "    data = text.split()\n",
            "    data = list(filter(lambda x: len(x) >= WORD_LENGTH_THRESHOLD, data))\n",
            "    return data\n",
            "\n",
            "\n",
            "def parse_pdf(filename: str):\n",
            "    \"\"\"\n",
            "    Read text from a PDF file.\n",
            "    Clean the text, tokenize it, and return as a list of tokens.\n",
            "    :param :\n",
            "    \"\"\"\n",
            "    fd = open(filename, \"rb\")\n",
            "    document = PDFDocument(fd)\n",
            "    viewer = SimplePDFViewer(fd)\n",
            "    output_strings = []\n",
            "    for i in range(len(list(document.pages()))):\n",
            "        viewer.navigate(1)\n",
            "        viewer.render()\n",
            "        output_strings.extend(viewer.canvas.strings)\n",
            "    file_contents = \" \".join(output_strings)\n",
            "    return clean_text(file_contents)\n",
            "\n",
            "\n",
            "def parse_resume_df():\n",
            "    resume_df = pd.read_csv(\"data/resume-dataset.csv\")\n",
            "    resume_df[\"Keywords\"] = resume_df[\"Resume\"].apply(clean_text)\n",
            "    return resume_df[\"Keywords\"].values, resume_df[\"Category\"].values\n"
          ]
        }
      ]
    }
  ]
}